{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656008bf-a4dc-4a3b-bf9a-5918f91fc667",
   "metadata": {},
   "source": [
    "# Assessment 2 \n",
    "During the covid pandemic, governments tried to predict the levels of infection so that they could manage the required infrastructure, especially hospital beds and (sadly) mortuary places. In this mini-project, you will take data from two different, publicly available, sources and try to see how well you could have done if you had been working for the UK government.  One of the sources is the UK data from the period and the other is google’s data on our behaviour during the pandemic.\n",
    "\n",
    "You will use data up to a specific date to predict hospital beds needed, and deaths that will occur  in one, two, three and four weeks from that date.  You will do this for the whole pandemic period i.e. using a rolling  window. You will also quantify how well your prediction works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a2185-3936-438d-a15d-cbb4e4d2ff95",
   "metadata": {},
   "source": [
    "Things that you might want to consider in your analysis include:\n",
    "\n",
    "- There will be a time lag between people’s  behaviour changing and the number of people requiring a hospital bed (and another between people requiring hospitalisation and them dying).\n",
    "\n",
    "- Both sources of data will have information that is  of no use to you and you will have to investigate those sources that are useful to you. You should start by thinking this through and then trying the effect of different variable.\n",
    "\n",
    "- Both data sets may require some cleaning and (perhaps) some smoothing to remove effects such as weekends etc.\n",
    "\n",
    "- As the situation changed some aspects of the older data nay or may not continue to be useful.  \n",
    "\n",
    "- You may wish to do this by region or for the whole country, or even to compare the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874be11-81f7-4611-8b1d-9e570970dbf4",
   "metadata": {},
   "source": [
    "We will provide you with basic code to access the data. It is up to you if you use it or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d374d4f-9428-4434-b223-a48e011f3279",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14af081-c9e8-45ac-ada6-3c62e1d58e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "#Show plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "#Some date formatters to use for plotting \n",
    "weeks = mdates.WeekdayLocator()\n",
    "years = mdates.YearLocator()\n",
    "months = mdates.MonthLocator()\n",
    "weekdays = mdates.DayLocator()\n",
    "dateFmt = mdates.DateFormatter('%b-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2c2007-9f92-4f42-ba5a-5c6a4fd5c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn is a useful package for handling graphics and \n",
    "#producing publication quality images with better colour schemes\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "#sns.set(font_scale=1.5) \n",
    "#sns.set_context(\"poster\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "plt.rcParams.update({'lines.linewidth':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7233c-c4cd-4730-88ff-4b1ea5358803",
   "metadata": {},
   "source": [
    "## 2. Predictors\n",
    "\n",
    "In principle, covid cases, hospital admissions, and deaths are time-lagged outcomes of covariates or \"predictor variables\" such as the level of interactions in the population, mobility metrics, levels of restrictions etc.\n",
    "\n",
    "A data set which is often used as a possible predictor is google mobility metrics https://www.google.com/covid19/mobility/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826f2919-f3b8-4b2c-9e09-24f05641cb95",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sm219/Downloads/Global_Mobility_Report.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import the data from Google.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#This is a large file!\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_google \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/sm219/Downloads/Global_Mobility_Report.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/Neural_Networks/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/Neural_Networks/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/Neural_Networks/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/Neural_Networks/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/Neural_Networks/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sm219/Downloads/Global_Mobility_Report.csv'"
     ]
    }
   ],
   "source": [
    "#Import the data from Google.\n",
    "#This is a large file!\n",
    "df_google = pd.read_csv('/home/sm219/Downloads/Global_Mobility_Report.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08988e-9df1-4e55-a019-18b9e4f14d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select `United Kingdom` from the full data.\n",
    "df_google = df_google[df_google['country_region']=='United Kingdom']\n",
    "#Discard sub regions\n",
    "#Note - we England vs UK\n",
    "df_google = df_google[df_google['sub_region_1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c8556-bd29-40c5-85c8-259b6c96ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-index by data\n",
    "df_google = df_google.set_index(pd.to_datetime(df_google['date']))\n",
    "df_google = df_google.sort_index()\n",
    "df_google.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a5e18-8f28-4a6d-a3ea-23ba4d9a15f7",
   "metadata": {},
   "source": [
    "Plot up some of the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3eff8-f576-4bf7-9399-0e3422cce5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10),sharex= True, facecolor='white')\n",
    "\n",
    "ax.plot(df_google.index,df_google['workplaces_percent_change_from_baseline'],label='Work')\n",
    "ax.plot(df_google.index,df_google['transit_stations_percent_change_from_baseline'],label='Public Transport')\n",
    "ax.plot(df_google.index,df_google['parks_percent_change_from_baseline'],label='Parks')\n",
    "\n",
    "\n",
    "ax.tick_params(axis=\"both\", direction=\"in\", which=\"both\", right=True,left=True, top=True, bottom=True)\n",
    "\n",
    "\n",
    "ax.grid(which='both')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Fraction relative to Pre-Covid')\n",
    "\n",
    "# format the ticks\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(dateFmt)\n",
    "ax.xaxis.set_minor_locator(weeks)\n",
    "#ax.set_xlim(left=dt.datetime(2021,7,1))\n",
    "ax.set_title('UK Workplace Google Metric')\n",
    "ax.legend()\n",
    "\n",
    "_= fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mobility.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0067ee-4d8c-44c0-b71b-fb3b88ac54fc",
   "metadata": {},
   "source": [
    "## The UK data from the covid period.\n",
    "\n",
    "The UK data is available as a zip file from:\n",
    "\n",
    "https://ukhsa-dashboard.data.gov.uk/covid-19-archive-data-download\n",
    "\n",
    "this then needs to be unzipped \n",
    "```console\n",
    "unzip  covid-19-archive.zip\n",
    "```\n",
    "This should produce a series of directories:\n",
    "```console\n",
    "drwxr-xr-x. 1 collngdj collngdj        126 Oct 25 16:08 Cases\n",
    "-rw-r--r--. 1 collngdj collngdj 1592889236 Oct 25 16:07 covid-19-archive.zip\n",
    "drwxr-xr-x. 1 collngdj collngdj        126 Oct 25 16:08 Deaths\n",
    "drwxr-xr-x. 1 collngdj collngdj        126 Oct 25 16:08 Healthcare\n",
    "drwxr-xr-x. 1 collngdj collngdj        126 Oct 25 16:08 Testing\n",
    "drwxr-xr-x. 1 collngdj collngdj        126 Oct 25 16:08 Vaccinations\n",
    "```\n",
    "\n",
    "each of these has a set of csv files arranged over different years.\n",
    "\n",
    "The following code is an example of how you might want to read these in and concatinate them over different years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9013f7-8024-44a1-8606-c5d29a9eb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from enum import StrEnum, auto\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "class DataType(StrEnum):\n",
    "    Vaccinations = auto()\n",
    "    Cases = auto()\n",
    "    Deaths = auto()\n",
    "    Healthcare = auto()\n",
    "    Testing = auto()\n",
    "    \n",
    "\n",
    "\n",
    "def csvconcat(datatype: DataType,\n",
    "              metric: str | None = None,\n",
    "              dataroot: str | pathlib.Path = pathlib.Path.cwd()) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get Pandas DataFrames from the UK COVID-19 .csv data, Concatenated across years.\n",
    "\n",
    "    Args:\n",
    "        datatype (DataType): Corresponds to the directory to scan use enum type above.\n",
    "                             i.e. [DataType.Cases|DataType.Deaths|DataType.Healthcare|\n",
    "                                   DataType.Testing|DataType.Vaccinations]\n",
    "        metric (str | None, optional): Chose an individual metric to process. If None (the default) then all\n",
    "                                       metrics for the given DataType are processed. Defaults to None.\n",
    "        dataroot (pathlib.Path, optional): The root directory for the unpacked UK COVID-19 data.\n",
    "                                           DataType directories (Cases/Deaths) etc should be under this root.\n",
    "                                           Defaults to pathlib.Path.cwd().\n",
    "    Raises:\n",
    "        ValueError: If there is a failure to convert the given dataroot to a Path object.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, pd.DataFrame]: The mapping from metric to fully concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataroot, pathlib.Path):\n",
    "        try:\n",
    "            dataroot = pathlib.Path(dataroot)\n",
    "        except:\n",
    "            raise ValueError(f\"dataroot: '{dataroot}' could not be converted to a Path.\")\n",
    "\n",
    "    dataroot /= datatype.name\n",
    "    metric_regex = re.compile(r\"(?P<metric>\\w+?)_(?P<specifier>nation|region|utla|ltla|overview)_20\\d\\d.csv\")\n",
    "    metrics: set[str] = {str(metric)}\n",
    "    if metric is None:\n",
    "        file_list = (file_.relative_to(dataroot).name for file_ in dataroot.rglob(\"*.csv\"))\n",
    "        metrics = {match.group(\"metric\") for file_ in file_list if (match := metric_regex.match(file_))}\n",
    "\n",
    "    ret = {}\n",
    "    for current_metric in metrics:\n",
    "        ret.update({current_metric: pd.concat(pd.read_csv(file_) for file_ in dataroot.rglob(f\"{current_metric}_*.csv\"))})\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93cd30-deef-4743-a5bd-299e66c3ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific metric\n",
    "path = \"/home/sm219/Downloads/covid-19-archive\"\n",
    "data = csvconcat(DataType.Healthcare, \"hospitalCases\", dataroot = path)\n",
    "print(list(data.keys()))\n",
    "type(data['hospitalCases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b1bf3-caa0-4975-b981-4d2c1d1bb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metrics for a DataType\n",
    "data = csvconcat(DataType.Healthcare, dataroot = path)\n",
    "print(f\"Data contains {len(data)} concatenated metrics\")\n",
    "#print(f\"Checking our chosen metric 'transmissionRateGrowthRateMax' is found in data: {'transmissionRateGrowthRateMax' in data}\")\n",
    "print(\"Available metrics:\")\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96431a-a401-4514-aa12-880f3777ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".custombg {\n",
    "    background-color: green;\n",
    "    color: white;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e3932-e7f1-4ae5-8be1-ba46c01424bf",
   "metadata": {},
   "source": [
    "# Start of project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa80112-393c-488e-9e4c-88fecd0f8a71",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Initially, the goal is to do some exploratory data analysis to get a feel of the different datasets and the various features and data types present in them. This is done using `df.head()` and `df.info()` to look at the data stored inside them. The prediction for the COVID beds and COVID deaths were done seperately because either of them could have been a predictor for the other one.\n",
    "\n",
    "**MARKER: Adjust `path` variable accordingly to load data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1ae47-bc09-42f1-90f2-c145788f5c39",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "Firstly, some of the datsets were examined and the data inside was plotted to get a feel for the dataset. Different metrics were plotted just by changing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcff6b4-b7f7-4bde-add5-3703fe288628",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = list(data.keys())[10]\n",
    "df_covid = data[metric]\n",
    "\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00a8fe-3d51-471b-a96a-656c0b300011",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Since we are doing time-series analysis it is better to set the datetime column as the index so it is easier to select data via dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac70bee-f03c-471c-9d23-f7594935f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-index by data\n",
    "df_covid.set_index(pd.to_datetime(df_covid['date']),inplace=True)\n",
    "df_covid.sort_index(inplace=True)\n",
    "df_covid.columns\n",
    "\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f934d5-ceee-4544-8625-89bec30def89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))  \n",
    "plt.plot(df_covid.index, df_covid[\"value\"], label = f\"{metric} data\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3da0d-2527-4119-980d-5c9f3193548c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "To allow our ML model to have the most predictive power, one of the most important steps is to feed the ML model the appropriate features so it can learn the complex relationships between the features and the target variable. Due to the high dimensionality and associated complexity of the data, the best approach will be to use Random Forest Regressor or XG Boost Regressor. These models have the capability to handle complex data whilst maintaining a managable computational cost. However, the list of all features from all the healthcare, testing, deaths, vaccinations, cases database is large. Therefore, to reduce the features down to a manageble level, some feature engineering has to be performed. Either LASSO or PCA can be used to reduce the dimensionality of the data. However, some features can already be removed based off requirements.\n",
    "\n",
    "### Initial Removed features\n",
    "\n",
    "**cum**: Any variable that are cumulative are irrelevant for the prediction of covid beds/deaths. The cumulative cases, or the cumulative admissions would not affect the current rolling prediction of covid beds/deaths. *Note: Exception applies to vaccination and potentially testing data where the cumulative vaccinations (1st dose/ 2nd dose etc.) would indicate a lower probability of (re)infection for those people. So all the cumulative data was preserved only in the vaccination and testing dataset.*\n",
    "\n",
    "**weekly**: All the weekly data was removed because the data was aggregated for the week thus making it irrelevant when there is daily data already available.\n",
    "\n",
    "**archive**: Any archived data was removed because those datapoints were from the past so not relevant when making current predictions.\n",
    "\n",
    "**publishdate**: In the testing datasets especially, many features had an equivalent publish date which was the date when the case/infection/death got recorded. Again, when the case/infection/death was regarded is irrelevant. Only the date of, say, death matters for predictions.\n",
    "\n",
    "**registrationdate**: Similar to publish date, registration date is another formality of when the data was recorded and again not relevant for us.\n",
    "\n",
    "**direction**: The direction features in the datasets indicate whether a certain feature is going up or down, which is not really important considering we already have numerically data. Furthermore, this will be highly correlated with the numerical data and provide no additional information but rather add unecessary features which will increase the computational complexity.\n",
    "\n",
    "**change/percentage**: This provides the change or percentage change which is ignored for now as we have the numerical data available already. This is a sort of 50/50 call to reduce the number of features. Latter we can try including these featurs to see the change that the features bring to the predicitve power of the model, if any.\n",
    "\n",
    "For vaccination and testing - two new considerations were included:\n",
    "\n",
    "**cum**: For vaccination and testing it does matter how many new people have been tested/vaccinated rather the total sum is more important. So cumulative features were left in.\n",
    "\n",
    "**new**: The daily change/the new number of people infected does not really matter. Only the total sum matters becaues it represents the people that are less likely to get infected. So any new change features were left out.\n",
    "\n",
    "For vaccinations, one final additon:\n",
    "\n",
    "**dose**: For vaccinations we are mainly interested in the total dosage given to people. The vaccinations and testing datasets are similar and probably closely correlated so we are only interested in a few features from the vaccination dataset such as the dosage.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020f4dd-577a-451e-bbea-410241a7894c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "A function called `transform_date` to group the data via date as we are considering national cases of covid rather than regional based data. We are mainly interested in the date and the value thus we can discard other columns such as the region and the metric name.\n",
    "\n",
    "Another function called `check_na` was used to make sure that the features used do not have *NaN* values for the date range that we are interested in.\n",
    "\n",
    "On another note, the processing for the data was done in batches to prevent loading up large files at the same time which caused memory issues resulting in jupyter notebooks crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffc564-1746-4e4f-8862-bbd2d4ffa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, metric_name, start = '2020-06-01', end = '2022-09-30', get_length = False):\n",
    "    \"\"\"\n",
    "    Retrieve the dataframe from the NHS dataset, aggregate the data by date, use datetime as the index,\n",
    "    and remove any archive data.\n",
    "\n",
    "    Parameters:\n",
    "    metric_name (str): A string to describe which metric for which we wish to create a dataframe.\n",
    "    data (dict): A dictionary containing the data.\n",
    "    start (Timestamp): A date-time value_cumCasesBySpecimenDate  852 format string to denote the start date of the data.\n",
    "    end (Timestamp): A date-time format string to denote the end date of the data.\n",
    "    get_length (Boolean): Returns the length of the metric instead of a dataframe.\n",
    "\n",
    "    Returns:\n",
    "    df_temp (Dataframe): A dataframe which is preprocessed such that there are no archives and the data is grouped by date.\n",
    "    \"\"\"\n",
    "    df_temp = data[metric_name]\n",
    "    \n",
    "    \n",
    "    if 'metric_name' in df_temp.columns:\n",
    "        df_temp['metric_name'] = df_temp['metric_name'].astype(str)  # Ensure it's string type\n",
    "        df_temp = df_temp[~df_temp['metric_name'].str.contains(\"archive\", case=False, na=False)]\n",
    "\n",
    "    \n",
    "    if 'date' in df_temp.columns:\n",
    "        df_temp['date'] = pd.to_datetime(df_temp['date'])\n",
    "        df_temp.set_index('date', inplace=True)\n",
    "    \n",
    "    \n",
    "    df_temp = df_temp.groupby('date')['value'].sum().reset_index() #collect all data\n",
    "    df_temp.set_index('date', inplace=True)\n",
    "\n",
    "    start_date = pd.Timestamp(start)\n",
    "    cutoff_date = pd.Timestamp(end)\n",
    "    #Only return the date range we are examining\n",
    "    df_temp = df_temp.loc[(df_temp.index >= start_date) & (df_temp.index <= cutoff_date)]\n",
    "\n",
    "    df_temp = df_temp.rename(columns={\"value\": f\"value_{metric_name}\"})  # Rename value column\n",
    "\n",
    "    if get_length:\n",
    "        return len(df_temp[f\"value_{metric_name}\"])\n",
    "    \n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20599009-98a9-4000-8340-37889bcf6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_na(data, metrics_list, target_length, start = '2020-06-01', end = '2022-09-30'):\n",
    "    '''\n",
    "    Loop through the dataframe the make sure the size of the data are as required.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): A dictionary containing the data.\n",
    "    metrics_list (list): A list of all the metric names in data.\n",
    "    target_length (int): An integer describing the required length of the dataframe.\n",
    "    start (Timestamp): A date-time format string to denote the start date of the data.\n",
    "    end (Timestamp): A date-time format string to denote the end date of the data.\n",
    "\n",
    "    Returns:\n",
    "    na_list (list): A list of columns to remove because they contain NaN values.\n",
    "    '''\n",
    "    na_list = []\n",
    "    start_date = pd.Timestamp(start)\n",
    "    cutoff_date = pd.Timestamp(end)\n",
    "\n",
    "    for metric in metrics_list:\n",
    "        df = data[metric]\n",
    "\n",
    "        if 'metric_name' in df.columns:\n",
    "            df['metric_name'] = df['metric_name'].astype(str) \n",
    "            df = df[~df['metric_name'].str.contains(\"archive\", case=False, na=False)]\n",
    "\n",
    "    \n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.set_index('date')\n",
    "        elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        df = df.groupby('date')['value'].sum().reset_index()\n",
    "        df = df.set_index('date')\n",
    "\n",
    "        df = df.loc[(df.index >= start_date) & (df.index <= cutoff_date)]\n",
    "\n",
    "        if \"value\" in df.columns:\n",
    "            total_entries = len(df[\"value\"])\n",
    "            \n",
    "\n",
    "            if (total_entries != target_length): #if there are too few datapoints remove those columns\n",
    "                na_list.append(metric)\n",
    "\n",
    "    return na_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fccd76-2e6a-4038-b8c7-248fe7f64b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthcare = csvconcat(DataType.Healthcare, dataroot = path) #Loading the data first\n",
    "data_cases = csvconcat(DataType.Cases, dataroot = path)\n",
    "data_deaths = csvconcat(DataType.Deaths, dataroot = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60329b0-c43a-407a-8920-634eda32962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_beds = transform_data(data = data_healthcare, metric_name = \"hospitalCases\", get_length = True) #get target beds length\n",
    "print(target_beds) #can use same value for beds and deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81aac0-1284-4327-a7e8-a9c233f6ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_col_list = check_na(data_healthcare, metrics_list = list(data_healthcare.keys()), target_length = target_beds)\n",
    "print(na_col_list)\n",
    "print(len(na_col_list))\n",
    "print(len(list(data_healthcare.keys()))) #checking if check_na works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d2f3c-2e44-40f9-9ef3-2672c58bc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_data(data_cases, list(data_cases.keys())[4]) #checking if transform_data works as inteded.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1b054-ad59-4a55-a753-b8851f156097",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data_healthcare, data_cases, data_deaths] #combining in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78f923-e3e5-4ba2-b3e7-1c28ca3ac8fe",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Combining all the dataframes together and discarding some obvious ones which are not going to be useful for our ML project as discussed above. To reduce the number of features, LASSO can be used to identify the most important features in the ML model. So the features that were ambigious as to whether they would help or not have been included. More advanced feature selection is done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff969a85-c23b-4f83-a69a-54a0ab4ad03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_datasets(data_list, unwanted_terms, target_column, wanted_terms = None):\n",
    "    \"\"\"\n",
    "    Processes multiple datasets to filter, transform, and concatenate them based on specified conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - data_list (list): A list of datasets (Dictionaries or DataFrames) to process.\n",
    "    - check_na (function): Function to check for non-NA percentage in datasets.\n",
    "    - transform_data (function): Function to transform data based on specified metrics.\n",
    "    - unwanted_terms (list): List of strings with terms to filter out unwanted metrics.\n",
    "    - target_column (str): Name of the target column for NA checks. \n",
    "\n",
    "    Returns:\n",
    "    - combined_df (DataFrame): A combined DataFrame after processing all metrics and datasets.\n",
    "    \"\"\"\n",
    "    dfs = []  # list of dataframes to concatenate\n",
    "\n",
    "    for selected_data in tqdm(data_list, desc=\"Processing DataTypes...\"):\n",
    "        metric_list = list(selected_data.keys())\n",
    "        na_col_list = check_na(data=selected_data, metrics_list=metric_list, target_length=target_beds)\n",
    "        \n",
    "        \n",
    "        filtered_metrics = [\n",
    "            metric for metric in metric_list\n",
    "            if not any(term in metric.lower() for term in unwanted_terms) and metric not in na_col_list and\n",
    "            (wanted_terms is None or any(term in metric.lower() for term in wanted_terms))\n",
    "        ]\n",
    "\n",
    "        \n",
    "        for metric in tqdm(filtered_metrics, desc='Processing metrics...'):\n",
    "            transformed_df = transform_data(data=selected_data, metric_name=metric)\n",
    "            \n",
    "            if transformed_df is not None:\n",
    "                dfs.append(transformed_df)\n",
    "\n",
    "    \n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdfb6e-62e0-45b8-8826-8e8a75acb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = process_datasets(\n",
    "    data_list=data_list,  \n",
    "    unwanted_terms=[\"cum\", \"change\", \"direction\", \"percentage\", \"weekly\", \"archive\", \"publishdate\", 'registrationdate'], \n",
    "    target_column='covidOccupiedMVBeds'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d85ea-b3cb-4470-9016-f985b6624fa4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Processing the data in batches to prevent the notebook from crashing by loading the vaccinations and testing data seperately. Also, we included cumulative features for testing and vaccinations because the total number of people with, for example, 1st dose of vaccination would imply they are less likely to get infected so less likely to be admitted to hospital for covid as discussed above. Thus, all the cumulative features were kept for testing and vaccinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb585a6-3914-42d7-8993-3a4a93fbf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_cases, data_deaths, data_list, data_healthcare #free up memeory to prevent notebook from crashing\n",
    "\n",
    "data_testing = csvconcat(DataType.Testing, dataroot = path)\n",
    "data_list = [data_testing]\n",
    "\n",
    "test_df = process_datasets(\n",
    "    data_list=data_list,  \n",
    "    unwanted_terms=['capacity',\"change\", \"direction\", \"percentage\", \"weekly\", \"archive\", \"publishdate\", 'registrationdate'], \n",
    "    target_column='covidOccupiedMVBeds'\n",
    ")\n",
    "\n",
    "del data_testing #clear memory\n",
    "\n",
    "combined_df = pd.concat([combined_df, test_df], axis=1)\n",
    "combined_df.to_csv('data_without_vacc.csv') #save file for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1ed8b-7ce5-4925-9800-e67abde1196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df = pd.read_csv('data_without_vacc.csv', index_col = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d1d80-b1bd-4315-a3a8-8e7a8883bc8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "**Marker: Please comment out cell below if any errors**\n",
    "\n",
    "*Note: Losing the vaccination data would not be too much of a hassle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891518d-fb66-450f-a798-43182fd34dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARKER: if notebook crashes: comment out this entire cell block\n",
    "\n",
    "data_vaccinations = csvconcat(DataType.Vaccinations, dataroot = path)\n",
    "\n",
    "vacc_df = process_datasets(\n",
    "    data_list=data_list,  \n",
    "    unwanted_terms=['capacity',\"change\", \"direction\", \"percentage\", \"weekly\", \"archive\", \"publishdate\", 'registrationdate'], \n",
    "    target_column='covidOccupiedMVBeds',\n",
    "    wanted_terms = ['cum', 'dose']\n",
    "    \n",
    ")\n",
    "\n",
    "del data_vaccinations #clear memory\n",
    "\n",
    "combined_df = pd.concat([combined_df, vacc_df], axis=1)\n",
    "combined_df.to_csv('data_with_vacc.csv') #save file for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c89e5-6f23-49f0-9d8b-c4f4c5c8d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0b5a5-0997-4a2c-a1ca-ca4e63554ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c02ab6-3b86-4891-b0b2-4c70275c4862",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now the data has been preprocessed, the google dataset can be combined with the original the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc69e3-93bb-4592-9176-012db7471344",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.Timestamp('2022-09-30') #only predicting upto here\n",
    "start_date = pd.Timestamp('2020-06-01')    \n",
    "\n",
    "df_google = df_google[(df_google.index >= start_date) & (df_google.index <= cutoff_date)] #match the dates with the covid data\n",
    "combined_df = combined_df[(combined_df.index >= start_date) & (combined_df.index <= cutoff_date)]\n",
    "\n",
    "#remove region labels on goolge dataframe\n",
    "for column in df_google.columns:\n",
    "    if \"percent\" not in column.lower():\n",
    "        df_google = df_google.drop([column], axis=1)\n",
    "\n",
    "combined_df = pd.concat([df_google, combined_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10beb5-7773-441c-bc75-0b372e440f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head() #check everything is in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94050ff-9506-4368-8564-0fc4835b4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting covid beds in timeframe we are interested\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(combined_df.index, combined_df['value_covidOccupiedMVBeds'], label='COVID Mechanical Ventilation Beds', color='royalblue', linewidth=2)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel('Number of Beds')\n",
    "plt.title('COVID-19 Mechanical Ventilation Bed Usage Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e816e-2aa7-4b45-b9d1-c71937f76134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same plot for the deaths\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(combined_df.index, combined_df['value_newDailyNsoDeathsByDeathDate'], label='COVID Deaths', color='red', linewidth=2)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel('Number of Deaths')\n",
    "plt.title('COVID-19 Deaths Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dd914-1f8d-4992-a651-b21f5538c397",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "## Additional features\n",
    "\n",
    "To allow the ML models to better predict the seasonal/periodic variations of the data, we can add three new features: day, month, year. The 'day' column will allow the model to understand weekend effects; the 'month' column will allow the model to understand seasonal variations and the year column will aid the model in understanding long time variations. Once, the LASSO is implemented in the next section it will become obvious whether any of these periodic features will be useful or not. These features need to be `OneHotEncoded`. This is done because ML algorithms usually work based on distance. So, for example, the 'days' column assigns 5 for Saturday and 6 for Sunday and 0 for Monday. Thus, the ML model would assume a bigger difference between Sunday and Monday compared to between Saturday and Sunday even though both days are consecutive days to Sunday. `OneHotEncoding` converts this categorical data to numerical data that the ML model can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395e53b-646d-4b12-a39d-7a93ca07687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ac1b9-aa35-4ee0-9600-c91dad79782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"day\"] = combined_df.index.dayofweek\n",
    "combined_df[\"month\"] = combined_df.index.month\n",
    "combined_df['year'] = combined_df.index.year\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output = False, handle_unknown='ignore')\n",
    "transform_df = combined_df[['day', 'month', 'year']]\n",
    "encoded_features = encoder.fit_transform(transform_df) #create encoded df\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_features, columns = encoder.get_feature_names_out(['day', 'month', 'year']), \n",
    "                          index = combined_df.index) #add them back to original dataframe\n",
    "\n",
    "combined_df = pd.concat([combined_df, encoded_df], axis=1)\n",
    "combined_df.drop(['day', 'month', 'year'], axis=1, inplace=True)\n",
    "\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8be40-7fbb-40fa-b8a6-c9be0f863791",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### LASSO feature reduction\n",
    "\n",
    "Now that the list of features has been shortened, a more sophisticated method can be employed to reduce the number of features further. This is required because if we were to input such high-dimensional data into a ML model, there will be a large risk of overfitting not to mention the large computational cost of performing such a process. Thus, `LASSO` is a linear regression model that introduces a parameter $\\lambda$ to he cost function of a ML model to punish it for choosing large scores. This allows `LASSO` to identify the best and worst features. These shortlisted features can then be used in further ML models.\n",
    "\n",
    "*Note: A rolling window `LASSO` was used to ensure the features identified were applicable for the whole time frame rather than just a particular point in time. Also, the window chosen was made short in the hope that the data would approximately be linear in this short window.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53459e8f-0b25-4c9a-bcdb-39717e9639fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def rolling_window_lasso(window_size, step_size, alpha=0.01, target_variable='value_covidOccupiedMVBeds'):\n",
    "    \"\"\"\n",
    "    Apply Lasso feature selection on a rolling window basis to identify important features\n",
    "    based on their coefficients, considering a regularisation parameter.\n",
    "\n",
    "    Parameters:\n",
    "    window_size (int): The size of the rolling window in days.\n",
    "    step_size (int): The step size for the rolling window.\n",
    "    alpha (float): The regularisation strength; must be a positive float.\n",
    "    target_variable (str): The name of the target variable to exclude from the Lasso model.\n",
    "\n",
    "    Returns:\n",
    "    feature_importance_df (pd.DataFrame): A DataFrame with average coefficients for the features across all windows.\n",
    "    \"\"\"\n",
    "\n",
    "    data_df = combined_df.drop([target_variable], axis=1)  # Exclude the target variable\n",
    "    target_df = combined_df[target_variable]  # Target variable data\n",
    "    feature_importance_df = pd.DataFrame(0, index=data_df.columns, columns=['Coefficient'])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Begin rolling window iteration\n",
    "    for start in range(0, len(data_df) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        window_data = data_df.iloc[start:end]\n",
    "        window_target = target_df.iloc[start:end]\n",
    "\n",
    "        window_data_scaled = scaler.fit_transform(window_data)\n",
    "        \n",
    "        # Applying Lasso\n",
    "        lasso = LassoCV(alphas=[alpha], cv=5, random_state=42)\n",
    "        lasso.fit(window_data_scaled, window_target)\n",
    "        feature_importance_df['Coefficient'] += np.abs(lasso.coef_)\n",
    "\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3432b-e063-4c56-a528-727b7d5ec1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE TO MARKER: THIS CELL WILL HAVE CONVERGENCE ISSUES SO BE CAREFUL WHEN RUNNING!!!!\n",
    "#lasso_features_df = rolling_window_lasso(window_size=5, step_size=1, alpha=0.01, target_variable='value_covidOccupiedMVBeds')\n",
    "#print(lasso_features_df.sort_values(by='Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815161c5-6171-4188-818c-cff8c2da7ea9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Principal Component Analysis (PCA) feature reduction\n",
    "\n",
    "Since the `LASSO` feature did not work, another dimensionality reduction feature called PCA can be implemented. This converts the vector space into another space with a linear combination of the features. Thus, the features with the highest coefficients can be selected which captures most of the variance of the data. Again, a rolling PCA window is used, whilst summing the feature coefficients to ensure that the selected features are important over the whole time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264837c-f083-49a6-9893-87e71939c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns  # For plotting stuff\n",
    "\n",
    "def rolling_window_pca(window_size, step_size, n_components=0.99, \n",
    "                       target_variable='value_covidOccupiedMVBeds',\n",
    "                       plot=False, num_features_to_plot=5, display_scatter_matrix=False, random_features=False):\n",
    "    \"\"\"\n",
    "    Apply PCA on a rolling window basis to calculate feature importance,\n",
    "    excluding the target variable and focusing on the overall features for modelling.\n",
    "\n",
    "    Parameters:\n",
    "    window_size (int): The size of the rolling window in days.\n",
    "    step_size (int): The step size for the rolling window.\n",
    "    n_components (float): The amount of variance that PCA should retain.\n",
    "    target_variable (str): The name of the target variable to exclude from the PCA.\n",
    "    plot (bool): If True, plot the importances of the specified number of features.\n",
    "    random_features (bool): If True, select features randomly for plotting instead of top features.\n",
    "    display_scatter_matrix (bool): If True, display a scatter matrix of the top features.\n",
    "    num_features_to_plot (int): Number of top features to display in the plot.\n",
    "\n",
    "    Returns:\n",
    "    feature_importance_df (DataFrame): A DataFrame with normalised importance scores for all features.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_df = combined_df.drop([target_variable], axis=1)  # Exclude the target variable\n",
    "    feature_importance_df = pd.DataFrame(0, index=data_df.columns, columns=['Importance'])\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Rolling window iteration\n",
    "    for start in range(0, len(data_df) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        window_data = data_df.iloc[start:end]\n",
    "\n",
    "        data_scaled = scaler.fit_transform(window_data)\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        pca.fit(data_scaled)\n",
    "        \n",
    "        # Sum pca scores across time\n",
    "        loadings_sum = np.sum(np.abs(pca.components_), axis=0)\n",
    "        feature_importance_df['Importance'] += loadings_sum\n",
    "\n",
    "    \n",
    "    feature_importance_df['Importance'] /= feature_importance_df['Importance'].max()\n",
    "\n",
    "    # Determine which features to plot\n",
    "    if random_features:\n",
    "        features_to_plot_df = feature_importance_df.sample(n=num_features_to_plot, random_state=42)\n",
    "    else:\n",
    "        features_to_plot_df = feature_importance_df.nlargest(num_features_to_plot, 'Importance')\n",
    "    \n",
    "    if plot:\n",
    "        features_to_plot_df.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(10, 8))\n",
    "        plt.title(f'{len(features_to_plot_df)} Features Importance')\n",
    "        plt.xlabel('Normalised Importance')\n",
    "        plt.show()\n",
    "\n",
    "    if display_scatter_matrix:\n",
    "        top_features_names = features_to_plot_df.index\n",
    "        scatter_matrix_df = data_df[top_features_names]\n",
    "        pair_plot = sns.pairplot(scatter_matrix_df, height = 5)\n",
    "\n",
    "        for ax in pair_plot.axes.flatten():\n",
    "            ax.set_xlabel(ax.get_xlabel(), rotation=90)\n",
    "            ax.set_ylabel(ax.get_ylabel(), rotation=0)\n",
    "            ax.yaxis.labelpad = 300\n",
    "            ax.xaxis.labelpad = 20\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return feature_importance_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4b062-c5a3-4351-ac22-10092dd8f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_df = rolling_window_pca(window_size=7, step_size=1, n_components=0.99,\n",
    "                                  target_variable='value_covidOccupiedMVBeds',\n",
    "                                  plot=True, num_features_to_plot=5, display_scatter_matrix=True, random_features=True)\n",
    "\n",
    "print('DEATH PLOTS BELOW:')\n",
    "\n",
    "top_features_df_deaths = rolling_window_pca(window_size=7, step_size=1, n_components=0.99,\n",
    "                                  target_variable='value_newDailyNsoDeathsByDeathDate',\n",
    "                                  plot=True, num_features_to_plot=5, display_scatter_matrix=True, random_features=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a6083-2015-44eb-bd04-326aac140244",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Additional Feature Engineering\n",
    "\n",
    "Once, the top features exlcuding the target feature of covid beds has been determined. Further feature engineering can be done. For example, just by inspection there are many variables above that are very similar/correlated but there is no reason to include all these features which represent the same thing. For example, the death tally over 28 days, 60 days or the death rate for those periods describe the same data but in different contexts. Having only one of them is enough to encapsulate the information whilst reducing the number of features at the same time. Inputting high dimensional data with highly correlated features would result in a ML model overfitting due to the computational complexity. We can check if any other features are highly correlated and remove them from the combined dataframe. This can be done to remove redudant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca48fa-ea77-418a-a147-6d993c73b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def remove_correlated_features(features_df, top_features_df, correlation_threshold=0.8, plot=False, num_features_to_plot=5,\n",
    "                              random_features=False):\n",
    "    \"\"\"\n",
    "    Identifies and removes highly correlated features based on the correlation threshold.\n",
    "    Optionally plots a correlation heatmap of the most important features based on PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    features_df (DataFrame): DataFrame containing the features to check.\n",
    "    top_features_df (DataFrame): DataFrame containing feature importances.\n",
    "    correlation_threshold (float): Threshold above which a pair of features is considered highly correlated.\n",
    "    plot (bool): If True, display a heatmap of the most correlated features.\n",
    "    num_features_to_plot (int): Number of features to display in the plot.\n",
    "    random_features (bool): If True, selects features randomly for plotting instead of top features.\n",
    "    \n",
    "    Returns:\n",
    "    reduced_features_df (DataFrame): DataFrame with less correlated features.\n",
    "    \"\"\"\n",
    "    #Calculating the correlation matrix\n",
    "    features_df = features_df[top_features_df.index]\n",
    "    corr_matrix = features_df.corr().abs()\n",
    "\n",
    "    if plot:\n",
    "        if random_features:\n",
    "            features_to_plot = pd.Series(top_features_df.index).sample(n=num_features_to_plot, random_state=42)\n",
    "        else:\n",
    "            features_to_plot = top_features_df['Importance'].nlargest(num_features_to_plot).index\n",
    "\n",
    "        sns.heatmap(corr_matrix.loc[features_to_plot, features_to_plot], cmap='coolwarm', cbar=True, linewidth= 0.5)\n",
    "        plt.title('Correlation Matrix for Selected Features')\n",
    "        plt.show()\n",
    "\n",
    "    # Identify features that are highly correlated to drop\n",
    "    to_drop = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if i == j:\n",
    "                continue #expect correlation between similar features\n",
    "            \n",
    "            if corr_matrix.iloc[i, j] >= correlation_threshold:\n",
    "                feature1 = corr_matrix.columns[i]\n",
    "                feature2 = corr_matrix.columns[j]\n",
    "                # Retain the more important feature\n",
    "                if top_features_df.loc[feature1, 'Importance'] > top_features_df.loc[feature2, 'Importance']:\n",
    "                    to_drop.append(feature1)\n",
    "                else:\n",
    "                    to_drop.append(feature2)\n",
    "\n",
    "    reduced_features_df = features_df.drop(columns=to_drop)\n",
    "    print(f\"Selected {len(reduced_features_df.columns)} features: {reduced_features_df.columns}\")\n",
    "    return reduced_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bded2f-472e-431d-bd47-55898d286d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = remove_correlated_features(features_df=combined_df, top_features_df=top_features_df,\n",
    "                                        correlation_threshold=0.8, plot=True, num_features_to_plot=5, random_features=True)\n",
    "\n",
    "reduced_df_deaths = remove_correlated_features(features_df=combined_df, top_features_df=top_features_df_deaths,\n",
    "                                        correlation_threshold=0.8, plot=False, num_features_to_plot=5, random_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494a365-ffa9-4b6a-a296-23dde24be00b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "From the correlation matrix, it is obvious that there are numerous features that are correlated. The most uncorrelated features are the google movement features which makes sense because those are unique and not found in any other dataset. This provides a sanity check that the PCA and correlation matrices are working as intended. Now we order the dataframe by importance and select the data based on importance from the PCA. This will aid in narrowing down the most important uncorrelated features.\n",
    "\n",
    "*Note to marker: In the correlation matrix heatmap, there may be whitespaces sometimes. This is not a cause for concern, all this implies is that the feature importance score for that particular feature from PCA was 0. Thus the correlation matrix calculated a bunch of NaNs leading to white spaces. This is not an issue because the features with low importance do not get selected for the ML regression model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd6e24-80f1-46f6-8ae3-1706a0c4b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_df.sort_values(by='Importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3636d59-3bf7-47a7-9278-d598e2b8f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features(data_df, importance_df, num_features=15, target='covidOccupiedMVBeds'):\n",
    "    \"\"\"\n",
    "    Selects the top columns from reduced_df based on the importance scores in top_features_df.\n",
    "\n",
    "    Parameters:\n",
    "    reduced_df (DataFrame): DataFrame with reduced features.\n",
    "    top_features_df (DataFrame): DataFrame containing the importance scores of features.\n",
    "    num_features (int): Number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    processed_df (DataFrame): DataFrame containing only the top features including the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_features = importance_df['Importance'].sort_values(ascending=False).index\n",
    "    available_features = [feature for feature in sorted_features if feature in data_df.columns] #check to see if it uncorrelated\n",
    "    top_features = available_features[:num_features]\n",
    "    processed_df = data_df[top_features]\n",
    "    \n",
    "    target = 'value_' + target\n",
    "    temp_df = combined_df[target]\n",
    "    processed_df = pd.concat([processed_df, temp_df], axis=1)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16810d9e-e064-44f3-8ec8-a3052c90d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_df = select_top_features(data_df = reduced_df, importance_df=top_features_df, num_features=15)\n",
    "\n",
    "ML_df_deaths = select_top_features(data_df = reduced_df_deaths, importance_df=top_features_df_deaths, num_features=15, target='newDailyNsoDeathsByDeathDate')\n",
    "\n",
    "ML_df.head() #check to see if everything in order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b35b9a-7ce9-4217-b33b-38d0ed5a7fd7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now the data preprocessing has been completed. We can finally move on to calculate the time lag between the features and the target variable. Although it is worth noting the important of the days for the dataset. Maybe it will be worth doing the ML model with and without the additionally included day/month/year features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085cc74-371c-443a-9dab-4d8bead76973",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Time lag between features\n",
    "There will be a time lag between features changing and the actual effects being visible on the number of COVID beds required. To quantify this time lag the cross correlation between the COVID beds and the different features can be used to determine the time lag for which there will be the maximum cross correlation. The maximum time lag determined via research is roughly 11 days. This allows us to search a narrower range of times allowing for lower computational complexitiy. *Ref:* [Quantifying the Time-Lag Effects of Human\n",
    "Mobility on the COVID-19 Transmission:\n",
    "A Multi-City Study in China](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9262890). In this approach we will search for time lags of 100 days to be able to clearly observe the maximum shift although we are only expecting a shift of around 11 days.\n",
    "\n",
    "When conducting the lag we only need to look forward in time i.e. start searching from 0 days onwards because we are expecting the features to have an effect on the covid beds rather than the covid beds having an effect on the feature. Thus, only positive time lags are considered. Furthermore, if the correlation was found to be more than 30 days then the lag was set to 0 because in those cases there was no clear trend of the features affecting the covid beds.\n",
    "\n",
    "In addition, because the day/month/year should not have any correlation because the just represent time so the lag was set to 0 by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bb6fa-9fc7-401d-9c97-477fe3d3b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_correlation(feature, df, target='value_covidOccupiedMVBeds', max_lag=100, display_plot = False):\n",
    "    \"\"\"\n",
    "    Plot cross-correlation between a feature and a target variable over a range of lags.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the time series data\n",
    "    feature (str): Column name of the feature to correlate with the target\n",
    "    target (str): Column name of the target variable\n",
    "    max_lag (int): Maximum lag to compute the cross-correlation for\n",
    "\n",
    "    Returns:\n",
    "    optimal_lag (int): The optimal lag for that feature compared to the target\n",
    "    \"\"\"\n",
    "    \n",
    "    correlations = [] # List to store correlation coefficients\n",
    "\n",
    "    if feature == target:\n",
    "        return 0\n",
    "\n",
    "    if ('day' in feature) or ('month' in feature) or ('year' in feature):\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    for lag in range(0, max_lag + 1): #search postive range\n",
    "        shifted = df[feature].shift(lag)  # Shift the feature\n",
    "        corr = shifted.corr(df[target])   # Calculate correlation with the target\n",
    "        correlations.append(corr)\n",
    "\n",
    "    if display_plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(0, max_lag + 1), correlations, marker='.', label = \"Cross correlation\")\n",
    "        plt.axvline(x=0, color='k', linestyle='--', label='No Shift')\n",
    "        plt.title(f'Cross-Correlation\\nFeature: {feature} vs. {target}')\n",
    "        plt.xlabel('Lag (Days)')\n",
    "        plt.ylabel('Correlation Coefficient')\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc = \"best\")\n",
    "        plt.show()\n",
    "\n",
    "    max_corr = max(correlations)  # Consider the maximum by absolute value\n",
    "    optimal_lag = correlations.index(max_corr)\n",
    "    print(f\"Maximum correlation of {max_corr:.2f} occurs at lag {optimal_lag}.\")\n",
    "\n",
    "    if optimal_lag > 30: #the lag should not exceede one month\n",
    "        return 0\n",
    "\n",
    "    return optimal_lag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4beff6-c686-4656-8d34-3d0fdad66107",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_list = [plot_cross_correlation(feature, df = ML_df, display_plot=True) for feature in ML_df.columns]\n",
    "\n",
    "shift_list_deaths = [plot_cross_correlation(feature, df = ML_df_deaths, display_plot=False, \n",
    "                                            target = 'value_newDailyNsoDeathsByDeathDate') for feature in ML_df_deaths.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0717b-7387-465c-82eb-b61152cd5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shift_list) #check the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84dbbb-bdd9-4c32-91e8-d3c217bf8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_df(data_df, shifts):\n",
    "\n",
    "    shifted_df = pd.DataFrame(index=data_df.index)\n",
    "    for col, shift in zip(data_df.columns, shifts):\n",
    "        shifted_df[col] = data_df[col].shift(shift, fill_value=0) #shifting and replacing gaps with 0s\n",
    "\n",
    "    return shifted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d655c-137d-444c-a253-87de4b5f743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ML_df = shift_df(data_df=ML_df, shifts=shift_list) #check to see everything works\n",
    "\n",
    "final_ML_df_deaths = shift_df(data_df=ML_df_deaths, shifts=shift_list_deaths)\n",
    "\n",
    "final_ML_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4331b5-3c6e-4872-9d95-fa6720c8c20c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## ML Regression\n",
    "\n",
    "Now that the data has been preprocessed and some irrelevant features removed, the data can start to be inputted into an appropriate model. For this task, because of the complexity of the features and the non-linear relationships within the data it is probably best to use `RandomForest` or `XGBoost` to capture these complex relations between the features and the target variable. Additionally, `RandomForest` and `XGBoost` can deal with multi-dimensional data and has the ability to select the best features for the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf748c-de87-48a1-b6c5-fafab06fe573",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ML_df_deaths.isna().sum() #check all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811cff8-2639-43aa-8f13-fcf31e2685a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def rolling_window_regression(df, train_window, prediction_period, estimator, target_variable='value_covidOccupiedMVBeds', plot=False):\n",
    "    \"\"\"\n",
    "    Apply regression models on a rolling window basis, predicting the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the features and target variable.\n",
    "    train_window (int): The size of the training window in days.\n",
    "    prediction_period (int): The prediction period window length.\n",
    "    estimator (sklearn estimator): The regression model to be used.\n",
    "    target_variable (str): The name of the target variable.\n",
    "    plot (bool): If True, plot the actual vs predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    rmse (float): The Root Mean Squared Error of the predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    prediction_indices = []\n",
    "    \n",
    "    # Prepare the data\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    y = df[target_variable]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', estimator)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Rolling window iteration\n",
    "    for i in tqdm(range(0, len(df) - train_window - prediction_period + 1, prediction_period), desc=\"Model Training\"):\n",
    "\n",
    "        end_train = i + train_window\n",
    "        start_pred = end_train\n",
    "        end_pred = start_pred + prediction_period\n",
    "\n",
    "        \n",
    "        if end_pred > len(df):\n",
    "            end_pred = len(df)\n",
    "        \n",
    "        if end_train >= len(df):\n",
    "            continue\n",
    "        \n",
    "        #print(f\"end train: {end_train}, start_pred:{start_pred} and end pred:{end_pred}\")\n",
    "        \n",
    "        \n",
    "        train_X = X.iloc[i:end_train]\n",
    "        train_y = y.iloc[i:end_train]\n",
    "        test_X = X.iloc[start_pred:end_pred]\n",
    "        test_y = y.iloc[start_pred:end_pred]\n",
    "\n",
    "       \n",
    "        pipeline.fit(train_X, train_y)\n",
    "\n",
    "        \n",
    "        y_pred = pipeline.predict(test_X) #comparing to the actual data\n",
    "        predictions.extend(y_pred)\n",
    "        actuals.extend(test_y.values)\n",
    "        prediction_indices.extend(test_y.index)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions)) #best way to calculate rmse\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y.index, y, label='Actual')\n",
    "        plt.plot(prediction_indices, predictions, label='Predictions')\n",
    "        plt.axvline(x=y.index[train_window], color='r', linestyle='--', label='Start of predictions')\n",
    "        plt.title('Actual vs Predicted Values')\n",
    "        plt.xlabel('Date')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel(target_variable)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9144b-57b5-4fc0-965f-321a4f6395c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XG = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "rmse = rolling_window_regression(df=final_ML_df, train_window=14, prediction_period=7,\n",
    "                                 estimator=model_XG, plot=True)\n",
    "\n",
    "rmse_deaths = rolling_window_regression(df=final_ML_df_deaths, train_window=14, prediction_period=7,\n",
    "                                 estimator=model_XG, plot=True, target_variable='value_newDailyNsoDeathsByDeathDate')\n",
    "print(f'Root Mean Squared Error for Beds: {rmse:.2f} and for Deaths: {rmse_deaths:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de4bb5-1c8d-4d1d-8a82-77ae658de85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare with random forest to see which one is better\n",
    "model_RF = RandomForestRegressor(n_estimators=100, max_depth = 3, random_state=42)\n",
    "\n",
    "rmse = rolling_window_regression(df=final_ML_df, train_window=14, prediction_period=7,\n",
    "                                 estimator=model_RF, plot=True)\n",
    "\n",
    "rmse_deaths = rolling_window_regression(df=final_ML_df_deaths, train_window=14, prediction_period=7,\n",
    "                                 estimator=model_RF, plot=True, target_variable='value_newDailyNsoDeathsByDeathDate')\n",
    "\n",
    "print(f'Root Mean Squared Error for Beds: {rmse:.2f} and for Deaths: {rmse_deaths:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b8fd5-bb5c-4840-8638-9729f14e2f8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "From the scatter matrix plot previously done, when selecting most important features it was noticed that a lot of the features are highly skewed. Now to adjust this, a log transformation can be applied to make the feature data more uniform and gaussian-like distribution. This may improve the performace of the ML model. Below is the code to do this. The code has been written using `sklearn` packages such that the log transformer can be added to the `Pipeline` we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dee00-0022-4ca4-854a-bac1d45b4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class custom_log_transformer: #making custom class to put into Pipeline\n",
    "\n",
    "    '''\n",
    "        Custom transformation class that takes the log of the data. \n",
    "\n",
    "        Methods:\n",
    "        fit: allow the log transformer to fit the data\n",
    "        transform: allow the log transformer to change the data to log values\n",
    "    '''\n",
    "\n",
    "    def __init__(self, add_one=True):\n",
    "        self.add_one = add_one\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.add_one:\n",
    "            return np.log(X)\n",
    "        else:\n",
    "            return np.log(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b27b78-3d28-4753-9f4e-7092066d78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see if this improves the model in any way\n",
    "\n",
    "\n",
    "\n",
    "def rolling_window_regression_with_log(df, train_window, prediction_period, estimator, \n",
    "                                       target_variable='value_covidOccupiedMVBeds', plot=False):\n",
    "    \"\"\"\n",
    "    Apply regression models on a rolling window basis, predicting the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the features and target variable.\n",
    "    train_window (int): The size of the training window in days.\n",
    "    prediction_period (int): The prediction period window length.\n",
    "    estimator (sklearn estimator): The regression model to be used.\n",
    "    target_variable (str): The name of the target variable.\n",
    "    plot (bool): If True, plot the actual vs predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    rmse (float): The Root Mean Squared Error of the predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    prediction_indices = []  # Store the indices of predictions\n",
    "\n",
    "    # Prepare the data\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    y = df[target_variable]\n",
    "\n",
    "    pipeline = Pipeline([ \n",
    "        ('scaler', StandardScaler()),\n",
    "        ('log', custom_log_transformer()), #added in log transformer\n",
    "        ('regressor', estimator)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Rolling window iteration\n",
    "    for i in tqdm(range(0, len(df) - train_window - prediction_period + 1, prediction_period), desc=\"Model Training\"):\n",
    "\n",
    "        end_train = i + train_window\n",
    "        start_pred = end_train\n",
    "        end_pred = start_pred + prediction_period\n",
    "\n",
    "        \n",
    "        if end_pred > len(df):\n",
    "            end_pred = len(df)\n",
    "        \n",
    "        if end_train >= len(df):\n",
    "            continue\n",
    "        \n",
    "        #print(f\"end train: {end_train}, start_pred:{start_pred} and end pred:{end_pred}\")\n",
    "        \n",
    "        \n",
    "        train_X = X.iloc[i:end_train]\n",
    "        train_y = y.iloc[i:end_train]\n",
    "        test_X = X.iloc[start_pred:end_pred]\n",
    "        test_y = y.iloc[start_pred:end_pred]\n",
    "\n",
    "       \n",
    "        pipeline.fit(train_X, train_y)\n",
    "\n",
    "        \n",
    "        y_pred = pipeline.predict(test_X) #comparing to the actual data\n",
    "        predictions.extend(y_pred)\n",
    "        actuals.extend(test_y.values)\n",
    "        prediction_indices.extend(test_y.index)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y.index, y, label='Actual')\n",
    "        #prediction_start_index = y.index[train_window + prediction_period - 2:] \n",
    "        #prediction_start_index = y.index[len(predictions)]\n",
    "        plt.plot(prediction_indices, predictions, label='Predictions')\n",
    "        plt.axvline(x=y.index[train_window + prediction_period - 1], color='r', linestyle='--', label='Start of predictions')\n",
    "        plt.title('Actual vs Predicted Values')\n",
    "        plt.xlabel('Date')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel(target_variable)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae39930-e925-4d54-a31e-28c0d11ecb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transform did not work!\n",
    "\n",
    "#model_RF = RandomForestRegressor(n_estimators=1000, max_depth = 3, random_state=42)\n",
    "\n",
    "#rmse = rolling_window_regression_with_log(df=final_ML_df, train_window=14, prediction_period=7,\n",
    "                                 #estimator=model_RF, plot=True)\n",
    "\n",
    "#rmse_deaths = rolling_window_regression_wth_log(df=final_ML_df_deaths, train_window=14, prediction_period=7,\n",
    "                                 #estimator=model_RF, plot=True, target_variable='value_newDailyNsoDeathsByDeathDate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b6e20-99c1-4ed9-ae72-cecc3fd8cd6a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "*Note: Tried to implement the log transformer, but there were a lot of values close to 0 that were becoming inf or too large to store in float64 arrays. Thus, this approach was abandoned.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff83b08-a571-4e3e-a4d8-4d83c28d3220",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "From the above examples, it can be seen that the `Random Forest Regressor` performs slighly better than the `XGBoost Regressor`. Thus, we can safely proceede with the `Random Forest Regressor` from here onwards. The `Random Forest Regressor` can be fine tuned in a couple of ways: firstly, hyperparameter tuning can be done to ensure that the model is fine tuned for this particular dataset. Secondly, the training and testing period above of 14 days and 7 days respectively was chosen arbitrarily so this can be changed to see the optimal training and testing periods for the rolling window. Finally, it is worth removing the days/months/years column or adding in more features to see if the model performance increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17f5df-5bd0-4a5b-b863-4f8bf678169f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now the training window and the prediction period needs to be optimised such that the training window is not too far back in the future such that the model learns trends from previous times and the prediction period short enough such that the trends do not deviate too much from the model predictions. For these parts it would be ideal to use the HPC cluster to fine tune through all the combinations but in the interest of time we can perform a rough search outlined below. Also in the interest of time, the window was optimised just for one model instead of both. This will half the number of computations required.\n",
    "\n",
    "*Note: Feel free to try and optimise the windows for both models, the code is in there.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26543af-2624-422a-965a-c696df076610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimising the training window and prediction period\n",
    "train_window_list = [i for i in range(14, 43, 7)]\n",
    "prediction_period_list = [i for i in range(1, 8, 2)]\n",
    "\n",
    "best_rmse = 1e9 #some large values\n",
    "#best_rmse_deaths = 1e9 \n",
    "\n",
    "ideal_train_window = 0\n",
    "ideal_prediction_period = 0\n",
    "\n",
    "#ideal_train_window_deaths = 0\n",
    "#ideal_prediction_period_deaths = 0\n",
    "\n",
    "model_RF = RandomForestRegressor(n_estimators=100, random_state=42) #simpler model to speed up process\n",
    "\n",
    "for train_window in train_window_list:\n",
    "    for prediction_period in prediction_period_list:\n",
    "        rmse = rolling_window_regression(df=final_ML_df, train_window=train_window, prediction_period=prediction_period,\n",
    "                                 estimator=model_RF, plot=False)\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            ideal_train_window = train_window\n",
    "            ideal_prediction_period = prediction_period\n",
    "\n",
    "        #rmse_deaths = rolling_window_regression(df=final_ML_df_deaths, train_window=train_window, prediction_period=prediction_period,\n",
    "                                 #estimator=model_RF, plot=False, target_variable='value_newDailyNsoDeathsByDeathDate')\n",
    "\n",
    "        # if rmse_deaths < best_rmse_deaths:\n",
    "        #     best_rmse_deaths = rmse_deaths\n",
    "        #     ideal_train_window = train_window\n",
    "        #     ideal_prediction_period = prediction_period\n",
    "\n",
    "print(f'Best rmse is {best_rmse:.2f} for training window: {ideal_train_window} and prediction period: {ideal_prediction_period}')\n",
    "#print(f'COVID Deaths - best rmse is {best_rmse_deaths} for training window: {ideal_train_widnow_deaths} and prediction period: {ideal_prediction_period_deaths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db0c79-1f8b-47a8-9cc3-467574860904",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now that the optimal training window and prediction window has been determined, we can proceede to fine-tune the hyperparameters for the model. Again, in the interest of time we can loop over some of the parameters, coarsely rather than do some fine search for the optimal hyperparameters. Ideally, we would have used `GridSearchCV` to search the parameter space but in our case a simple nested for loop will suffice. So, for `Random Forest Regressor` there are many hyperparameters that can be tuned, but the most important ones are the n_estimators and the max_depth. Again, tuning for the beds and then applying to the covid deaths. Not ideal, but concept is same. This approach is only used to save computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c86c12-41f9-40f1-90e0-f485edaf1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list = [100, 200, 300]\n",
    "max_depth_list = [3,4,5] #small grid for less computational cost. Not ideal.\n",
    "\n",
    "# n_estimators_list = [i for i in range(100, 2000, 250)]\n",
    "# max_depth_list = [i for i in range(1,10)] #better search, can use this if got high computation power.\n",
    "\n",
    "final_rmse = 1e9\n",
    "ideal_n_estimators = 0\n",
    "ideal_max_depth = 0\n",
    "\n",
    "# final_rmse_deaths = 1e9\n",
    "# ideal_n_estimators_deaths = 0\n",
    "# ideal_max_depth_deaths = 0\n",
    "\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        new_model_RF = RandomForestRegressor(n_estimators=n_estimators, max_depth = max_depth, random_state=42, n_jobs=-1)\n",
    "        rmse = rolling_window_regression(df=final_ML_df, train_window=ideal_train_window, prediction_period=ideal_prediction_period,\n",
    "                                 estimator=new_model_RF, plot=False)\n",
    "\n",
    "        if rmse < final_rmse:\n",
    "            final_rmse = rmse\n",
    "            ideal_n_estimators = n_estimators\n",
    "            ideal_max_depth = max_depth\n",
    "\n",
    "        \n",
    "        #rmse_deaths = rolling_window_regression(df=final_ML_df, train_window=ideal_train_window, prediction_period=ideal_prediction_period,\n",
    "                                 #estimator=model_RF, plot=False, target_variable='value_newDailyNsoDeathsByDeathDate')\n",
    "        \n",
    "\n",
    "        # if rmse_deaths < final_rmse_deaths:\n",
    "        #     final_rmse_deaths = rmse_deaths\n",
    "        #     ideal_n_estimators_deaths = n_estimators\n",
    "        #     ideal_max_depth_deaths = max_depth\n",
    "\n",
    "print(f'The best rmse is {final_rmse:.2f} for best max depth: {ideal_max_depth} and the best n estimators:{ideal_n_estimators}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f85870-b286-4392-8231-15e2d2e940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now it is time to do the final plot with our model\n",
    "\n",
    "model_RF = RandomForestRegressor(n_estimators=ideal_n_estimators, max_depth = ideal_max_depth, random_state=42)\n",
    "\n",
    "rmse = rolling_window_regression(df=final_ML_df, train_window=ideal_train_window, prediction_period=ideal_prediction_period,\n",
    "                                 estimator=model_RF, plot=True)\n",
    "\n",
    "rmse_deaths = rolling_window_regression(df=final_ML_df_deaths, train_window=ideal_train_window, prediction_period=ideal_prediction_period,\n",
    "                                 estimator=model_RF, plot=True, target_variable='value_newDailyNsoDeathsByDeathDate')\n",
    "\n",
    "print(f'Root Mean Squared Error for Beds: {rmse:.2f} and for Deaths: {rmse_deaths:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac976-0d51-44db-9b78-c9bfe1e8189b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "All in all, the model predicted the number of COVID beds and deaths required fairly well given the data. Ideally, with more computing power, the training and testing window and the hyperparameters could have been found more accurately. To improve the code, all the data preprocessing and the training could have been combined into one ML pipeline that can be run on all the data. Furthermore, we could have compared the PCA reduction with a basic Random Forest feature importance selection to cross-check whether PCA was picking up the most important results or not.\n",
    "\n",
    "Given more time, some of the features could have been combined or multiplied together to see how the results are affected. In addition, more feature could have incorporated by examining the optimal boundary to set the cross-correlation threshold when deciding if a dataset is correlated or not. This could have been done by varying the cross-correlation threshold and observing how the ML model improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c91e9f-13cb-4836-aa8a-fcb1bcd63fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a621047-3216-4c32-83f4-0318d91e1676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba59df-e65b-4792-8ce1-ac2e33755d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701990ce-e096-43cf-a40a-8cd8ce204017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
